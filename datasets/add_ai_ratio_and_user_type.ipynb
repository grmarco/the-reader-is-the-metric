{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo actualizado guardado en: ../1_2_datasets_final/2_trained_models/by_user/rf_san_valentin/ttcw_model_results.csv\n",
      "Archivo actualizado guardado en: ../1_2_datasets_final/2_trained_models/by_user/rf_san_valentin/confederacy_model_results.csv\n",
      "Archivo actualizado guardado en: ../1_2_datasets_final/2_trained_models/by_user/rf_san_valentin/pronvsprompt_model_results.csv\n",
      "Archivo actualizado guardado en: ../1_2_datasets_final/2_trained_models/by_user/rf_san_valentin/slm_model_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ruta al archivo de preferencias\n",
    "user_pref_path = \"user_preferences.csv\"\n",
    "df_pref = pd.read_csv(user_pref_path)\n",
    "\n",
    "# Diccionario: nombre de archivo -> valor de Dataset en user_preferences\n",
    "files_datasets = {\n",
    "    \"ttcw_model_results.csv\": \"ttcw\",\n",
    "    \"confederacy_model_results.csv\": \"confederacy\",\n",
    "    \"pronvsprompt_model_results.csv\": \"pronvsprompt\",\n",
    "    \"slm_model_results.csv\": \"slm\",\n",
    "    \"hanna_model_results.csv\": \"hanna\"\n",
    "}\n",
    "\n",
    "base_results_path = \"../1_2_datasets_final/2_trained_models/by_user/rf_san_valentin\"\n",
    "\n",
    "for file_name, dataset in files_datasets.items():\n",
    "    file_path = os.path.join(base_results_path, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Eliminar cualquier columna que empiece por 'ratio_ai' o 'ai_preferred'\n",
    "    cols_to_drop = [col for col in df.columns if col.startswith(\"ratio_ai\") or col.startswith(\"ai_preferred\")]\n",
    "    if cols_to_drop:\n",
    "        df.drop(columns=cols_to_drop, inplace=True)\n",
    "    \n",
    "    # Filtrar las preferencias para el dataset indicado y seleccionar columnas necesarias\n",
    "    df_pref_ds = df_pref[df_pref[\"Dataset\"] == dataset][[\"User ID\", \"ratio_ai\"]].copy()\n",
    "    df_pref_ds.rename(columns={\"User ID\": \"user_id\"}, inplace=True)\n",
    "\n",
    "    # Unir (merge) en \"user_id\"\n",
    "    df_merged = pd.merge(df, df_pref_ds, on=\"user_id\", how=\"left\")\n",
    "    \n",
    "    # Si existe la columna \"thematic_analysis.inter_theme_similarity\", insertar \"ratio_ai\" a continuación\n",
    "    if \"ratio_ai\" in df_merged.columns:\n",
    "        if \"thematic_analysis.inter_theme_similarity\" in df_merged.columns:\n",
    "            idx = df_merged.columns.get_loc(\"thematic_analysis.inter_theme_similarity\")\n",
    "            ratio_col = df_merged.pop(\"ratio_ai\")\n",
    "            df_merged.insert(idx + 1, \"ratio_ai\", ratio_col)\n",
    "    \n",
    "        # Crear la columna \"ai_preferred\": 1 si ratio_ai > 0.5, de lo contrario 0.\n",
    "        ai_pref = (df_merged[\"ratio_ai\"] > 0.5).astype(int)\n",
    "        idx = df_merged.columns.get_loc(\"ratio_ai\")\n",
    "        df_merged.insert(idx + 1, \"ai_preferred\", ai_pref)\n",
    "    else:\n",
    "        print(f\"Advertencia: 'ratio_ai' no existe en {file_name} después del merge.\")\n",
    "\n",
    "    output_file = file_path#.replace(\".csv\", \"_with_ratio.csv\")\n",
    "    df_merged.to_csv(output_file, index=False)\n",
    "    print(f\"Archivo actualizado guardado en: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los CSV\n",
    "user_preferences = pd.read_csv('/data/gmarco/canon_explicability/1_2_datasets_final/user_preferences.csv')\n",
    "assessment_poems = pd.read_csv('/data/gmarco/canon_explicability/1_1_datasets_preparation/porter/output/assessment_poems.csv')\n",
    "\n",
    "# Filtrar los ResponseId únicos y seleccionar las columnas deseadas\n",
    "assessment_unique = assessment_poems[['ResponseId', 'Background in poetry', 'Frequency', 'Like poetry']].drop_duplicates(subset='ResponseId')\n",
    "\n",
    "# Renombrar columnas para facilitar el merge:\n",
    "# * 'ResponseId' → 'User ID' (clave para el merge)\n",
    "# * 'Background in poetry' → 'reader_type'\n",
    "assessment_unique = assessment_unique.rename(columns={\n",
    "    'ResponseId': 'User ID',\n",
    "    'Background in poetry': 'reader_type'\n",
    "})\n",
    "\n",
    "# Combinar usando merge left\n",
    "merged = pd.merge(user_preferences, assessment_unique, on='User ID', how='left')\n",
    "\n",
    "# Guardar el resultado en un nuevo CSV\n",
    "merged.to_csv('/data/gmarco/canon_explicability/1_2_datasets_final/user_preferences_with_reader_type.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Umbral (mediana) de ratio_ai por reader_type: 0.5\n",
      "Mapping de reader_type a reader_expertise: {'I am a professional poet': 'expert', 'I am familiar with multiple poets and genres of poetry': 'lay reader', 'I am not very familiar with poetry': 'lay reader', 'I have read a lot of poetry, but have never written poetry': 'lay reader', 'I have written poetry, but not professionally': 'lay reader'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el CSV\n",
    "df = pd.read_csv('/data/gmarco/canon_explicability/1_2_datasets_final/user_preferences_with_reader_type.csv')\n",
    "\n",
    "# Verificar si la columna 'reader_type' existe\n",
    "if 'reader_type' not in df.columns:\n",
    "    raise KeyError(\"La columna 'reader_type' no existe en el DataFrame\")\n",
    "\n",
    "# Calcular el promedio de ratio_ai para cada categoría en reader_type\n",
    "reader_stats = df.groupby('reader_type')['ratio_ai'].mean().reset_index()\n",
    "\n",
    "# Definir el umbral objetivamente, por ejemplo, la mediana de esos promedios\n",
    "threshold = 0.5\n",
    "print(\"Umbral (mediana) de ratio_ai por reader_type:\", threshold)\n",
    "\n",
    "# Mapear cada categoría: si el promedio es mayor o igual que el umbral -> 'lay reader', de lo contrario -> 'expert'\n",
    "reader_stats['reader_expertise'] = np.where(reader_stats['ratio_ai'] >= threshold, 'lay reader', 'expert')\n",
    "\n",
    "# Crear un diccionario para mapear los valores originales de reader_type\n",
    "mapping = dict(zip(reader_stats['reader_type'], reader_stats['reader_expertise']))\n",
    "print(\"Mapping de reader_type a reader_expertise:\", mapping)\n",
    "\n",
    "# Actualizar la columna reader_expertise sin sobreescribir los datos existentes:\n",
    "if 'reader_expertise' in df.columns:\n",
    "    mapped = df['reader_type'].map(mapping)\n",
    "    df['reader_expertise'] = df['reader_expertise'].combine_first(mapped)\n",
    "else:\n",
    "    df['reader_expertise'] = df['reader_type'].map(mapping)\n",
    "\n",
    "# Definir el mapeo según Dataset\n",
    "dataset_mapping = {\n",
    "    'hanna': 'lay reader',\n",
    "    'slm': 'lay reader',\n",
    "    'ttcw': 'expert',\n",
    "    'pronvsprompt': 'expert',\n",
    "    'confederacy': 'expert'\n",
    "}\n",
    "\n",
    "# Aplicar el mapeo y actualizar la columna reader_expertise sin sobreescribir los datos existentes:\n",
    "mapped_dataset = df['Dataset'].map(dataset_mapping)\n",
    "df['reader_expertise'] = df['reader_expertise'].combine_first(mapped_dataset)\n",
    "\n",
    "# Eliminar las columnas 'reader_type', 'Frequency' y 'Like poetry'\n",
    "df = df.drop(columns=['reader_type', 'Frequency', 'Like poetry'])\n",
    "\n",
    "# Guardar el resultado\n",
    "df.to_csv('/data/gmarco/canon_explicability/1_2_datasets_final/user_preferences_with_reader_expertise.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo actualizado guardado en: ../1_2_datasets_final/2_trained_models/by_user/rf_san_valentin/ttcw_model_results.csv\n",
      "Archivo actualizado guardado en: ../1_2_datasets_final/2_trained_models/by_user/rf_san_valentin/confederacy_model_results.csv\n",
      "Archivo actualizado guardado en: ../1_2_datasets_final/2_trained_models/by_user/rf_san_valentin/pronvsprompt_model_results.csv\n",
      "Archivo actualizado guardado en: ../1_2_datasets_final/2_trained_models/by_user/rf_san_valentin/slm_model_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Ruta al archivo de preferencias con reader_expertise\n",
    "user_pref_path = \"/data/gmarco/canon_explicability/1_2_datasets_final/user_preferences_with_reader_expertise.csv\"\n",
    "df_pref = pd.read_csv(user_pref_path)\n",
    "\n",
    "for file_name, dataset in files_datasets.items():\n",
    "    file_path = os.path.join(base_results_path, file_name)\n",
    "    df = pd.read_csv(file_path)\n",
    "    if 'reader_expertise' in df.columns:\n",
    "        df = df.drop(columns=['reader_expertise'])\n",
    "    # Filtrar las preferencias para el dataset indicado\n",
    "    df_pref_ds = df_pref[df_pref[\"Dataset\"] == dataset][[\"User ID\", \"reader_expertise\"]].copy()\n",
    "    df_pref_ds.rename(columns={\"User ID\": \"user_id\"}, inplace=True)\n",
    "    \n",
    "    # Unir (merge) en \"user_id\"\n",
    "    df_merged = pd.merge(df, df_pref_ds, on=\"user_id\", how=\"left\")\n",
    "    mapping = {'lay reader': 0, 'expert': 1}\n",
    "    df_merged['expert'] = df_merged['reader_expertise'].map(mapping)\n",
    "    output_file = file_path\n",
    "    df_merged.to_csv(output_file, index=False)\n",
    "    print(f\"Archivo actualizado guardado en: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canonenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
