{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importances for Interpretable Preference Profiles\n",
    "\n",
    "Once a Random Forest model is trained for each reader $r_j$, we extract feature importance values to determine which attributes were most influential in distinguishing $\\mathbf{x}_i$ from $\\mathbf{x}_{i'}$. Specifically, we use the mean decrease in impurity (MDI), implemented in Scikit-learn, which measures how much each feature reduces the weighted impurity (Gini index or entropy) across all trees in the forest. These importance values serve as proxies for the weights $w_{jf}$ in the additive utility model, providing an interpretable representation of the reader’s preference structure.\n",
    "\n",
    "Summarizing or clustering these importance vectors across many readers helps reveal broader patterns. For instance, different subsets of readers (e.g., literary experts vs.\\ non-experts) may prioritize distinct sets of textual attributes, reflecting variations in evaluative criteria and reading strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of clusters: 2\n",
      "Visualizing clusters with PCA...\n",
      "Plotting confusion matrix...\n",
      "Computing and plotting cluster means...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import f_oneway\n",
    "import os\n",
    "from scipy.spatial import ConvexHull\n",
    "from matplotlib import rcParams\n",
    "from config import selected_features \n",
    "\n",
    "# Create output folder for graphs\n",
    "output_folder = \"figures/readers_clusters\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Function to load and process a single dataset\n",
    "def load_and_process_dataset(file_path, dataset_name):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_filtered = df[selected_features].copy()\n",
    "    df_filtered = df_filtered.apply(pd.to_numeric, errors='coerce')\n",
    "    df_filtered = df_filtered.dropna()\n",
    "    df_filtered['dataset'] = dataset_name\n",
    "    return df_filtered\n",
    "\n",
    "# Function to combine multiple datasets\n",
    "def combine_datasets(file_paths):\n",
    "    combined_data = pd.DataFrame()\n",
    "    for file_path, dataset_name in file_paths:\n",
    "        try:\n",
    "            data = load_and_process_dataset(file_path, dataset_name)\n",
    "            combined_data = pd.concat([combined_data, data], ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    return combined_data\n",
    "\n",
    "# Function to find the optimal number of clusters\n",
    "def find_optimal_clusters(data, max_clusters=10):\n",
    "    distortions = []\n",
    "    silhouettes = []\n",
    "    K = range(2, max_clusters + 1)\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        cluster_labels = kmeans.fit_predict(data)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "        silhouettes.append(silhouette_score(data, cluster_labels))\n",
    "    # Save Elbow Method plot\n",
    "    plt.figure(figsize=(3.5, 2.5))\n",
    "    plt.plot(K, distortions, 'bx-')\n",
    "    plt.xlabel(\"Number of Clusters (k)\", fontsize=10)\n",
    "    plt.ylabel(\"Distortion (Inertia)\", fontsize=10)\n",
    "    plt.title(\"Elbow Method\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/elbow_method.pdf\")\n",
    "    plt.close()\n",
    "    # Save Silhouette Score plot\n",
    "    plt.figure(figsize=(3.5, 2.5))\n",
    "    plt.plot(K, silhouettes, 'rx-')\n",
    "    plt.xlabel(\"Number of Clusters (k)\", fontsize=10)\n",
    "    plt.ylabel(\"Silhouette Score\", fontsize=10)\n",
    "    plt.title(\"Silhouette Score\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/silhouette_score.pdf\")\n",
    "    plt.close()\n",
    "    optimal_k = K[np.argmax(silhouettes)]\n",
    "    return optimal_k\n",
    "\n",
    "# Function to perform clustering\n",
    "def perform_clustering(data, n_clusters):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    # Aplicar PCA para reducir a 2 dimensiones antes del clustering\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(pca_data)\n",
    "    return cluster_labels, pca_data\n",
    "\n",
    "# Function to calculate and plot cluster means with heatmap\n",
    "def compute_and_plot_cluster_means(data, cluster_labels):\n",
    "    numeric_data = data.select_dtypes(include=[np.number]).copy()\n",
    "    numeric_data['Cluster'] = cluster_labels\n",
    "    # Calcular medias por cluster\n",
    "    cluster_means = numeric_data.groupby('Cluster').mean()\n",
    "    cluster_means.to_csv(f\"outputs/reader_cluster_means.csv\")\n",
    "    # Crear el heatmap con medias de clusters\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    ax = sns.heatmap(\n",
    "        data=cluster_means.T,   # Transponer para poner características en eje x\n",
    "        cmap=\"YlGnBu\",\n",
    "        cbar=True,\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        linewidths=0.5,\n",
    "        annot_kws={\"size\": 14}\n",
    "    )\n",
    "    ax.set_title(\"Cluster Means Heatmap\", fontsize=16)\n",
    "    ax.set_xlabel(\"Clusters\", fontsize=16)\n",
    "    ax.set_ylabel(\"Features\", fontsize=16)\n",
    "    ax.tick_params(axis='x', labelsize=16, rotation=0)\n",
    "    ax.tick_params(axis='y', labelsize=16, rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/feature_means_heatmap_inverted.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(cluster_labels, datasets):\n",
    "    df = pd.DataFrame({\"Cluster\": cluster_labels, \"Dataset\": datasets})\n",
    "    confusion_mat = pd.crosstab(df[\"Dataset\"], df[\"Cluster\"], rownames=[\"Dataset\"], colnames=[\"Cluster\"], normalize='index')\n",
    "    plt.figure(figsize=(3.5, 2.5))\n",
    "    sns.heatmap(confusion_mat, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", cbar=True)\n",
    "    plt.xlabel(\"Cluster\", fontsize=10)\n",
    "    plt.ylabel(\"Dataset\", fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/confusion_matrix.pdf\")\n",
    "    plt.close()\n",
    "    return confusion_mat\n",
    "\n",
    "def filter_outliers(features, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Filtra outliers usando el método del rango intercuartílico (IQR).\n",
    "    Devuelve una máscara booleana: True si el punto NO es outlier.\n",
    "    \"\"\"\n",
    "    Q1 = np.percentile(features, 25, axis=0)\n",
    "    Q3 = np.percentile(features, 75, axis=0)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    mask = np.all((features >= lower_bound) & (features <= upper_bound), axis=1)\n",
    "    return mask\n",
    "\n",
    "# Visualización con PCA, Convex Hulls y Cluster Regions (configurable para eliminar outliers)\n",
    "def visualize_clusters_pca_with_regions_high_contrast(data, cluster_labels, datasets, remove_outliers=True, outlier_threshold=1.5):\n",
    "    # Reducir dimensiones usando PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "    # Calcular la máscara de inliers usando IQR si se solicita\n",
    "    inlier_mask = filter_outliers(reduced_data, threshold=outlier_threshold) if remove_outliers else np.ones(len(reduced_data), dtype=bool)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # Paletas de colores\n",
    "    dataset_colors = plt.cm.tab20.colors\n",
    "    dataset_color_map = {dataset: dataset_colors[i % len(dataset_colors)] for i, dataset in enumerate(datasets.unique())}\n",
    "    cluster_colors = sns.color_palette(\"pastel\", len(np.unique(cluster_labels)))\n",
    "    # Dibujar regiones de clusters (usando solo puntos inliers)\n",
    "    for cluster in np.unique(cluster_labels):\n",
    "        cluster_mask = (cluster_labels == cluster) & inlier_mask\n",
    "        cluster_points = reduced_data[cluster_mask]\n",
    "        if len(cluster_points) > 2:  # ConvexHull requiere al menos 3 puntos\n",
    "            try:\n",
    "                hull = ConvexHull(cluster_points)\n",
    "                hull_vertices = cluster_points[hull.vertices]\n",
    "                plt.fill(hull_vertices[:, 0],\n",
    "                         hull_vertices[:, 1],\n",
    "                         color=cluster_colors[cluster],\n",
    "                         alpha=0.3,\n",
    "                         label=f\"Cluster {cluster} Region\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error en cluster {cluster}: {e}\")\n",
    "    # Dibujar puntos de cada dataset (filtrados de outliers si está activado)\n",
    "    for dataset in datasets.unique():\n",
    "        mask = (datasets == dataset) & inlier_mask\n",
    "        dataset_points = reduced_data[mask]\n",
    "        plt.scatter(dataset_points[:, 0],\n",
    "                    dataset_points[:, 1],\n",
    "                    label=f\"{dataset}\",\n",
    "                    color=dataset_color_map[dataset],\n",
    "                    alpha=0.9, edgecolor=\"k\", s=40)\n",
    "    plt.title(\"PCA Projection and Alusters of Reader Preference Vectors\", fontsize=12)\n",
    "    plt.xlabel(\"Principal Component 1\", fontsize=10)\n",
    "    plt.ylabel(\"Principal Component 2\", fontsize=10)\n",
    "    plt.legend(loc='best', bbox_to_anchor=(1, 1), fontsize=8)\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/pca_clusters_with_regions.png\", dpi=300)\n",
    "    plt.savefig(f\"{output_folder}/pca_clusters_with_regions.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "def visualize_clusters_pca_with_regions_high_contrast(data, cluster_labels, datasets, remove_outliers=True, outlier_threshold=1.5):\n",
    "    \"\"\"\n",
    "    Visualiza los clusters con PCA en 2D, usando Convex Hulls para mostrar las regiones de los clusters.\n",
    "    Optimizado para papers de ACL: compacto pero con texto grande y legible.\n",
    "    \"\"\"\n",
    "    # Reducir dimensiones usando PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "\n",
    "    # Filtrar outliers si está activado\n",
    "    inlier_mask = filter_outliers(reduced_data, threshold=outlier_threshold) if remove_outliers else np.ones(len(reduced_data), dtype=bool)\n",
    "\n",
    "    # Configuración del gráfico\n",
    "    fig, ax = plt.subplots(figsize=(5.5, 4))  # Compacto pero legible\n",
    "\n",
    "    # Paletas de colores mejor contrastadas\n",
    "    dataset_colors = plt.cm.tab10.colors\n",
    "    dataset_color_map = {dataset: dataset_colors[i % len(dataset_colors)] for i, dataset in enumerate(datasets.unique())}\n",
    "    cluster_colors = sns.color_palette(\"muted\", len(np.unique(cluster_labels)))\n",
    "\n",
    "    # Dibujar regiones de clusters (usando solo puntos inliers)\n",
    "    for cluster in np.unique(cluster_labels):\n",
    "        cluster_mask = (cluster_labels == cluster) & inlier_mask\n",
    "        cluster_points = reduced_data[cluster_mask]\n",
    "        if len(cluster_points) > 2:  # ConvexHull requiere al menos 3 puntos\n",
    "            try:\n",
    "                hull = ConvexHull(cluster_points)\n",
    "                hull_vertices = cluster_points[hull.vertices]\n",
    "                ax.fill(hull_vertices[:, 0],\n",
    "                        hull_vertices[:, 1],\n",
    "                        color=cluster_colors[cluster],\n",
    "                        alpha=0.3,\n",
    "                        label=f\"Cluster {cluster} Region\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error en cluster {cluster}: {e}\")\n",
    "\n",
    "    # Dibujar puntos de cada dataset (filtrados de outliers si está activado)\n",
    "    for dataset in datasets.unique():\n",
    "        mask = (datasets == dataset) & inlier_mask\n",
    "        dataset_points = reduced_data[mask]\n",
    "        ax.scatter(dataset_points[:, 0],\n",
    "                   dataset_points[:, 1],\n",
    "                   label=f\"{dataset}\",\n",
    "                   color=dataset_color_map[dataset],\n",
    "                   alpha=0.9,\n",
    "                   #edgecolor=\"black\",\n",
    "                   s=20)  # Tamaño de los puntos ajustado\n",
    "\n",
    "    # Configuración de etiquetas y título con fuentes más grandes\n",
    "    ax.set_title(\"PCA Projection of Readers Preference Vector \", fontsize=12)\n",
    "    ax.set_xlabel(\"PC1\", fontsize=10)\n",
    "    ax.set_ylabel(\"PC2\", fontsize=10)\n",
    "\n",
    "    # Leyenda compacta\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1, 1), fontsize=8, ncol=2)\n",
    "\n",
    "    # Estilo de la cuadrícula\n",
    "    ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "    # Ajustes finales y guardado\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/pca_clusters_with_regions_compact.pdf\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Configuración: activar/desactivar eliminación de outliers y ajustar umbral.\n",
    "    remove_outliers_config = False        # Cambiar a True para eliminar outliers\n",
    "    outlier_threshold_config = 1.5   \n",
    "    path_model_results = \"model_results/by_reader/rf\"      # Ajusta el umbral\n",
    "    file_paths = [\n",
    "        (f\"{path_model_results}/pronvsprompt_model_results.csv\", \"pronvsprompt\"),\n",
    "        (f\"{path_model_results}/slm_model_results.csv\", \"slm\"),\n",
    "        (f\"{path_model_results}/ttcw_model_results.csv\", \"ttcw\"),\n",
    "        (f\"{path_model_results}/hanna_model_results.csv\", \"hanna\"),\n",
    "        (f\"{path_model_results}/confederacy_model_results.csv\", \"confederacy\")\n",
    "    ]\n",
    "    combined_data = combine_datasets(file_paths)\n",
    "    numeric_columns = combined_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    combined_data_numeric = combined_data[numeric_columns + ['dataset']].copy()\n",
    "    optimal_k = find_optimal_clusters(combined_data_numeric.drop(columns=['dataset']))\n",
    "    print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "    optimal_k = 2\n",
    "    cluster_labels, pca_data = perform_clustering(combined_data_numeric.drop(columns=['dataset']), optimal_k)\n",
    "    print(\"Visualizing clusters with PCA...\")\n",
    "    visualize_clusters_pca_with_regions_high_contrast(pca_data,\n",
    "                                                      cluster_labels,\n",
    "                                                      combined_data_numeric['dataset'],\n",
    "                                                      remove_outliers=remove_outliers_config,\n",
    "                                                      outlier_threshold=outlier_threshold_config)\n",
    "    print(\"Plotting confusion matrix...\")\n",
    "    plot_confusion_matrix(cluster_labels, combined_data_numeric['dataset'])\n",
    "    print(\"Computing and plotting cluster means...\")\n",
    "    compute_and_plot_cluster_means(combined_data_numeric, cluster_labels)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "canonenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
